{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "add527d1-2e52-4f67-8a98-6e06de83211e",
      "metadata": {
        "id": "add527d1-2e52-4f67-8a98-6e06de83211e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "34321f38-8983-4630-a452-23934300aa27",
      "metadata": {
        "id": "34321f38-8983-4630-a452-23934300aa27",
        "outputId": "460a9029-6c88-4f99-8d89-4872e9dd4a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17532300.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 482347.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3793096.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4164050.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='data',train=True, transform=transform,download=True)\n",
        "test_dataset = datasets.MNIST(root='data',train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MOXsoRlL-yKL",
        "outputId": "cac30329-6ca8-4019-b331-bc00f09fff6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MOXsoRlL-yKL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd808da-7b7f-49fc-9209-37498cea2e4d",
      "metadata": {
        "id": "edd808da-7b7f-49fc-9209-37498cea2e4d",
        "outputId": "4144e442-6104-4325-a39f-b5a7cf927825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "\n",
            "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([256])\n",
            "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n"
          ]
        }
      ],
      "source": [
        "# Checking the dataset\n",
        "print('Training Set:\\n')\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.size())\n",
        "    print('Image label dimensions:', labels.size())\n",
        "    print(labels[:10])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc19b970-cf75-46d7-97f3-ef3b00644ef3",
      "metadata": {
        "id": "fc19b970-cf75-46d7-97f3-ef3b00644ef3"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "630899e6-24ca-400a-a521-e8b52a38ac1e",
      "metadata": {
        "id": "630899e6-24ca-400a-a521-e8b52a38ac1e"
      },
      "outputs": [],
      "source": [
        "# Custom reshape class\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self,*args):\n",
        "        super(Reshape,self).__init__()\n",
        "        self.shape = args\n",
        "    def forward(self,x):\n",
        "        with_new_shape = x.view(self.shape)\n",
        "        return with_new_shape\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self,*args):\n",
        "        super(Trim, self).__init__()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x[:, :, :28, :28]\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE,self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,32,3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(32,64,3,stride=2,padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64,64, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64,64, 3, stride=1, padding=1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.z_mean = torch.nn.Linear(3136,2)\n",
        "        self.z_log_var = torch.nn.Linear(3136,2)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2,3136),\n",
        "            Reshape(-1,64,7,7),\n",
        "            nn.ConvTranspose2d(64,64, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(64,64,3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(64,32,3, stride=2, padding=0),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(32,1,3 ,stride=1, padding=0),\n",
        "            Trim(),\n",
        "            nn.Sigmoid()\n",
        "\n",
        "\n",
        "        )\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        eps = torch.randn(mean.size(0),mean.size(1)).to(mean.get_device() )\n",
        "        sigma = torch.exp(log_var/2.0)\n",
        "        z = mean+sigma*eps\n",
        "        return z\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=  x.view((-1,1,28,28))\n",
        "        encoded=self.encoder(x)\n",
        "        mean = self.z_mean(encoded)\n",
        "        log_variance = self.z_log_var(encoded)\n",
        "        encoded_z  = self.reparameterize(mean, log_variance)\n",
        "        decoded=self.decoder(encoded_z)\n",
        "        return encoded, mean, log_variance, decoded\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2b5011-cb5d-43e7-9031-7d3e94959106",
      "metadata": {
        "id": "0d2b5011-cb5d-43e7-9031-7d3e94959106"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ecc37475-c8f5-4d79-8912-fdfde10ff57c",
      "metadata": {
        "id": "ecc37475-c8f5-4d79-8912-fdfde10ff57c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, _ in data_loader:\n",
        "            features = features.to(device)\n",
        "            logits = model(features)\n",
        "            loss = loss_fn(logits, features, reduction='sum')\n",
        "            num_examples += features.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9806d036-4231-498d-b8d4-872c5e221219",
      "metadata": {
        "id": "9806d036-4231-498d-b8d4-872c5e221219"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "log_dict = {'train_combined_loss_per_epoch': [],\n",
        "           'train_combined_loss_per_batch':[],\n",
        "           'train_reconstruction_loss_per_batch':[],\n",
        "           'train_kl_loss_per_batch':[]}\n",
        "loss_fn = F.mse_loss\n",
        "start_time = time.time()\n",
        "def train_vae(num_epochs, model, optimizer, device, train_loader,\n",
        "             loss_fn=None,\n",
        "             logging_interval=100,\n",
        "             skip_epoch_stats=False,\n",
        "             reconstruction_term_weight=1,\n",
        "             save_model=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, _) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "\n",
        "            _, mean, log_var, reconstruction=model(features)\n",
        "            # print(mean.get_device())\n",
        "\n",
        "            # KL Div Loss\n",
        "            kl_loss=torch.sum(-0.5*(1+log_var - mean**2 - torch.exp(log_var)),axis=1)\n",
        "            batch_size=kl_loss.size(0)\n",
        "\n",
        "            kl_loss_avg = kl_loss.mean()\n",
        "             ## Reconstruction Loss\n",
        "            if loss_fn is None:\n",
        "              loss_fn = F.mse_loss\n",
        "            # print(reconstruction.get_device())\n",
        "            # print(features.get_device())\n",
        "            reconstruction_loss = loss_fn(reconstruction, features,reduction='none')\n",
        "            reconstruction_loss = reconstruction_loss.view(batch_size, -1).sum(axis=1)\n",
        "            reconstruction_loss=reconstruction_loss.mean()\n",
        "\n",
        "            total_loss = reconstruction_loss+kl_loss_avg\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # LOGGING\n",
        "            log_dict['train_combined_loss_per_batch'].append(total_loss.item())\n",
        "            log_dict['train_reconstruction_loss_per_batch'].append(reconstruction_loss.item())\n",
        "            log_dict['train_kl_loss_per_batch'].append(kl_loss_avg.item())\n",
        "\n",
        "            if not batch_idx % logging_interval:\n",
        "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
        "                          % (epoch+1, num_epochs, batch_idx,\n",
        "                              len(train_loader), total_loss))\n",
        "        if not skip_epoch_stats:\n",
        "                model.eval()\n",
        "\n",
        "                with torch.set_grad_enabled(False):  # save memory during inference\n",
        "\n",
        "                    train_loss = compute_epoch_loss_autoencoder(\n",
        "                        model, train_loader, loss_fn, device)\n",
        "                    print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
        "                          epoch+1, num_epochs, train_loss))\n",
        "                    log_dict['train_combined_per_epoch'].append(train_loss.item())\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    if save_model is not None:\n",
        "        torch.save(model.state_dict(), save_model)\n",
        "\n",
        "    return log_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f128837-2bf0-47b7-affb-718e104ca3f7",
      "metadata": {
        "id": "4f128837-2bf0-47b7-affb-718e104ca3f7",
        "outputId": "8d18c098-0d12-408f-ad12-ff677333b6d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/050 | Batch 0000/0235 | Loss: 225.4168\n",
            "Epoch: 001/050 | Batch 0050/0235 | Loss: 58.2663\n",
            "Epoch: 001/050 | Batch 0100/0235 | Loss: 52.5989\n",
            "Epoch: 001/050 | Batch 0150/0235 | Loss: 52.4529\n",
            "Epoch: 001/050 | Batch 0200/0235 | Loss: 51.2659\n",
            "Time elapsed: 1.84 min\n",
            "Epoch: 002/050 | Batch 0000/0235 | Loss: 46.5115\n",
            "Epoch: 002/050 | Batch 0050/0235 | Loss: 47.1010\n",
            "Epoch: 002/050 | Batch 0100/0235 | Loss: 43.1624\n",
            "Epoch: 002/050 | Batch 0150/0235 | Loss: 44.6075\n",
            "Epoch: 002/050 | Batch 0200/0235 | Loss: 46.3845\n",
            "Time elapsed: 2.01 min\n",
            "Epoch: 003/050 | Batch 0000/0235 | Loss: 42.8215\n",
            "Epoch: 003/050 | Batch 0050/0235 | Loss: 44.8058\n",
            "Epoch: 003/050 | Batch 0100/0235 | Loss: 41.0650\n",
            "Epoch: 003/050 | Batch 0150/0235 | Loss: 43.3317\n",
            "Epoch: 003/050 | Batch 0200/0235 | Loss: 44.7489\n",
            "Time elapsed: 2.18 min\n",
            "Epoch: 004/050 | Batch 0000/0235 | Loss: 41.4757\n",
            "Epoch: 004/050 | Batch 0050/0235 | Loss: 43.4222\n",
            "Epoch: 004/050 | Batch 0100/0235 | Loss: 40.2434\n",
            "Epoch: 004/050 | Batch 0150/0235 | Loss: 42.7269\n",
            "Epoch: 004/050 | Batch 0200/0235 | Loss: 43.6821\n",
            "Time elapsed: 2.35 min\n",
            "Epoch: 005/050 | Batch 0000/0235 | Loss: 40.8056\n",
            "Epoch: 005/050 | Batch 0050/0235 | Loss: 42.6157\n",
            "Epoch: 005/050 | Batch 0100/0235 | Loss: 39.4469\n",
            "Epoch: 005/050 | Batch 0150/0235 | Loss: 41.5768\n",
            "Epoch: 005/050 | Batch 0200/0235 | Loss: 42.8957\n",
            "Time elapsed: 2.53 min\n",
            "Epoch: 006/050 | Batch 0000/0235 | Loss: 39.7145\n",
            "Epoch: 006/050 | Batch 0050/0235 | Loss: 42.4160\n",
            "Epoch: 006/050 | Batch 0100/0235 | Loss: 38.9435\n",
            "Epoch: 006/050 | Batch 0150/0235 | Loss: 41.3770\n",
            "Epoch: 006/050 | Batch 0200/0235 | Loss: 42.0154\n",
            "Time elapsed: 2.70 min\n",
            "Epoch: 007/050 | Batch 0000/0235 | Loss: 39.5643\n",
            "Epoch: 007/050 | Batch 0050/0235 | Loss: 41.9310\n",
            "Epoch: 007/050 | Batch 0100/0235 | Loss: 38.2324\n",
            "Epoch: 007/050 | Batch 0150/0235 | Loss: 40.7591\n",
            "Epoch: 007/050 | Batch 0200/0235 | Loss: 41.8610\n",
            "Time elapsed: 2.87 min\n",
            "Epoch: 008/050 | Batch 0000/0235 | Loss: 38.8036\n",
            "Epoch: 008/050 | Batch 0050/0235 | Loss: 41.7709\n",
            "Epoch: 008/050 | Batch 0100/0235 | Loss: 37.9685\n",
            "Epoch: 008/050 | Batch 0150/0235 | Loss: 39.9908\n",
            "Epoch: 008/050 | Batch 0200/0235 | Loss: 41.4610\n",
            "Time elapsed: 3.04 min\n",
            "Epoch: 009/050 | Batch 0000/0235 | Loss: 38.7500\n",
            "Epoch: 009/050 | Batch 0050/0235 | Loss: 41.2129\n",
            "Epoch: 009/050 | Batch 0100/0235 | Loss: 37.6301\n",
            "Epoch: 009/050 | Batch 0150/0235 | Loss: 39.6454\n",
            "Epoch: 009/050 | Batch 0200/0235 | Loss: 41.3853\n",
            "Time elapsed: 3.21 min\n",
            "Epoch: 010/050 | Batch 0000/0235 | Loss: 38.3385\n",
            "Epoch: 010/050 | Batch 0050/0235 | Loss: 41.0789\n",
            "Epoch: 010/050 | Batch 0100/0235 | Loss: 37.4627\n",
            "Epoch: 010/050 | Batch 0150/0235 | Loss: 39.7140\n",
            "Epoch: 010/050 | Batch 0200/0235 | Loss: 40.5701\n",
            "Time elapsed: 3.38 min\n",
            "Epoch: 011/050 | Batch 0000/0235 | Loss: 38.1757\n",
            "Epoch: 011/050 | Batch 0050/0235 | Loss: 40.7470\n",
            "Epoch: 011/050 | Batch 0100/0235 | Loss: 36.9396\n",
            "Epoch: 011/050 | Batch 0150/0235 | Loss: 39.1635\n",
            "Epoch: 011/050 | Batch 0200/0235 | Loss: 40.5612\n",
            "Time elapsed: 3.55 min\n",
            "Epoch: 012/050 | Batch 0000/0235 | Loss: 37.5041\n",
            "Epoch: 012/050 | Batch 0050/0235 | Loss: 40.7329\n",
            "Epoch: 012/050 | Batch 0100/0235 | Loss: 36.9540\n",
            "Epoch: 012/050 | Batch 0150/0235 | Loss: 39.1244\n",
            "Epoch: 012/050 | Batch 0200/0235 | Loss: 40.5789\n",
            "Time elapsed: 3.71 min\n",
            "Epoch: 013/050 | Batch 0000/0235 | Loss: 37.1820\n",
            "Epoch: 013/050 | Batch 0050/0235 | Loss: 40.6728\n",
            "Epoch: 013/050 | Batch 0100/0235 | Loss: 36.8185\n",
            "Epoch: 013/050 | Batch 0150/0235 | Loss: 39.0176\n",
            "Epoch: 013/050 | Batch 0200/0235 | Loss: 40.4119\n",
            "Time elapsed: 3.89 min\n",
            "Epoch: 014/050 | Batch 0000/0235 | Loss: 37.0937\n",
            "Epoch: 014/050 | Batch 0050/0235 | Loss: 40.4548\n",
            "Epoch: 014/050 | Batch 0100/0235 | Loss: 36.6407\n",
            "Epoch: 014/050 | Batch 0150/0235 | Loss: 38.6034\n",
            "Epoch: 014/050 | Batch 0200/0235 | Loss: 40.1609\n",
            "Time elapsed: 4.06 min\n",
            "Epoch: 015/050 | Batch 0000/0235 | Loss: 36.7565\n",
            "Epoch: 015/050 | Batch 0050/0235 | Loss: 40.1756\n",
            "Epoch: 015/050 | Batch 0100/0235 | Loss: 36.8259\n",
            "Epoch: 015/050 | Batch 0150/0235 | Loss: 38.2993\n",
            "Epoch: 015/050 | Batch 0200/0235 | Loss: 40.0780\n",
            "Time elapsed: 4.23 min\n",
            "Epoch: 016/050 | Batch 0000/0235 | Loss: 36.8848\n",
            "Epoch: 016/050 | Batch 0050/0235 | Loss: 39.9382\n",
            "Epoch: 016/050 | Batch 0100/0235 | Loss: 36.7473\n",
            "Epoch: 016/050 | Batch 0150/0235 | Loss: 38.1520\n",
            "Epoch: 016/050 | Batch 0200/0235 | Loss: 39.6911\n",
            "Time elapsed: 4.39 min\n",
            "Epoch: 017/050 | Batch 0000/0235 | Loss: 36.5053\n",
            "Epoch: 017/050 | Batch 0050/0235 | Loss: 40.1386\n",
            "Epoch: 017/050 | Batch 0100/0235 | Loss: 36.3651\n",
            "Epoch: 017/050 | Batch 0150/0235 | Loss: 38.2700\n",
            "Epoch: 017/050 | Batch 0200/0235 | Loss: 39.6591\n",
            "Time elapsed: 4.57 min\n",
            "Epoch: 018/050 | Batch 0000/0235 | Loss: 36.2046\n",
            "Epoch: 018/050 | Batch 0050/0235 | Loss: 39.7667\n",
            "Epoch: 018/050 | Batch 0100/0235 | Loss: 36.3981\n",
            "Epoch: 018/050 | Batch 0150/0235 | Loss: 37.7998\n",
            "Epoch: 018/050 | Batch 0200/0235 | Loss: 39.5243\n",
            "Time elapsed: 4.74 min\n",
            "Epoch: 019/050 | Batch 0000/0235 | Loss: 36.5851\n",
            "Epoch: 019/050 | Batch 0050/0235 | Loss: 39.9785\n",
            "Epoch: 019/050 | Batch 0100/0235 | Loss: 36.5221\n",
            "Epoch: 019/050 | Batch 0150/0235 | Loss: 38.0507\n",
            "Epoch: 019/050 | Batch 0200/0235 | Loss: 39.6498\n",
            "Time elapsed: 4.92 min\n",
            "Epoch: 020/050 | Batch 0000/0235 | Loss: 36.3180\n",
            "Epoch: 020/050 | Batch 0050/0235 | Loss: 39.6629\n",
            "Epoch: 020/050 | Batch 0100/0235 | Loss: 36.1679\n",
            "Epoch: 020/050 | Batch 0150/0235 | Loss: 37.6480\n",
            "Epoch: 020/050 | Batch 0200/0235 | Loss: 39.4765\n",
            "Time elapsed: 5.10 min\n",
            "Epoch: 021/050 | Batch 0000/0235 | Loss: 36.4530\n",
            "Epoch: 021/050 | Batch 0050/0235 | Loss: 39.5173\n",
            "Epoch: 021/050 | Batch 0100/0235 | Loss: 36.0352\n",
            "Epoch: 021/050 | Batch 0150/0235 | Loss: 37.9148\n",
            "Epoch: 021/050 | Batch 0200/0235 | Loss: 39.1186\n",
            "Time elapsed: 5.27 min\n",
            "Epoch: 022/050 | Batch 0000/0235 | Loss: 35.9569\n",
            "Epoch: 022/050 | Batch 0050/0235 | Loss: 39.7265\n",
            "Epoch: 022/050 | Batch 0100/0235 | Loss: 36.2055\n",
            "Epoch: 022/050 | Batch 0150/0235 | Loss: 37.5885\n",
            "Epoch: 022/050 | Batch 0200/0235 | Loss: 38.7971\n",
            "Time elapsed: 5.44 min\n",
            "Epoch: 023/050 | Batch 0000/0235 | Loss: 35.7101\n",
            "Epoch: 023/050 | Batch 0050/0235 | Loss: 39.5822\n",
            "Epoch: 023/050 | Batch 0100/0235 | Loss: 35.9088\n",
            "Epoch: 023/050 | Batch 0150/0235 | Loss: 37.6114\n",
            "Epoch: 023/050 | Batch 0200/0235 | Loss: 39.3175\n",
            "Time elapsed: 5.62 min\n",
            "Epoch: 024/050 | Batch 0000/0235 | Loss: 35.9666\n",
            "Epoch: 024/050 | Batch 0050/0235 | Loss: 39.6166\n",
            "Epoch: 024/050 | Batch 0100/0235 | Loss: 35.9994\n",
            "Epoch: 024/050 | Batch 0150/0235 | Loss: 37.3890\n",
            "Epoch: 024/050 | Batch 0200/0235 | Loss: 39.0367\n",
            "Time elapsed: 5.78 min\n",
            "Epoch: 025/050 | Batch 0000/0235 | Loss: 35.4326\n",
            "Epoch: 025/050 | Batch 0050/0235 | Loss: 39.2757\n",
            "Epoch: 025/050 | Batch 0100/0235 | Loss: 35.7583\n",
            "Epoch: 025/050 | Batch 0150/0235 | Loss: 37.4147\n",
            "Epoch: 025/050 | Batch 0200/0235 | Loss: 39.0785\n",
            "Time elapsed: 5.95 min\n",
            "Epoch: 026/050 | Batch 0000/0235 | Loss: 35.3969\n",
            "Epoch: 026/050 | Batch 0050/0235 | Loss: 39.2116\n",
            "Epoch: 026/050 | Batch 0100/0235 | Loss: 36.1186\n",
            "Epoch: 026/050 | Batch 0150/0235 | Loss: 37.4354\n",
            "Epoch: 026/050 | Batch 0200/0235 | Loss: 38.9149\n",
            "Time elapsed: 6.12 min\n",
            "Epoch: 027/050 | Batch 0000/0235 | Loss: 35.6518\n",
            "Epoch: 027/050 | Batch 0050/0235 | Loss: 39.4807\n",
            "Epoch: 027/050 | Batch 0100/0235 | Loss: 35.8271\n",
            "Epoch: 027/050 | Batch 0150/0235 | Loss: 37.2640\n",
            "Epoch: 027/050 | Batch 0200/0235 | Loss: 39.2174\n",
            "Time elapsed: 6.29 min\n",
            "Epoch: 028/050 | Batch 0000/0235 | Loss: 35.5443\n",
            "Epoch: 028/050 | Batch 0050/0235 | Loss: 39.1404\n",
            "Epoch: 028/050 | Batch 0100/0235 | Loss: 35.6587\n",
            "Epoch: 028/050 | Batch 0150/0235 | Loss: 37.0751\n",
            "Epoch: 028/050 | Batch 0200/0235 | Loss: 38.7802\n",
            "Time elapsed: 6.46 min\n",
            "Epoch: 029/050 | Batch 0000/0235 | Loss: 35.5454\n",
            "Epoch: 029/050 | Batch 0050/0235 | Loss: 38.9329\n",
            "Epoch: 029/050 | Batch 0100/0235 | Loss: 35.7080\n",
            "Epoch: 029/050 | Batch 0150/0235 | Loss: 37.0342\n",
            "Epoch: 029/050 | Batch 0200/0235 | Loss: 38.8307\n",
            "Time elapsed: 6.62 min\n",
            "Epoch: 030/050 | Batch 0000/0235 | Loss: 35.2573\n",
            "Epoch: 030/050 | Batch 0050/0235 | Loss: 38.8637\n",
            "Epoch: 030/050 | Batch 0100/0235 | Loss: 35.5628\n",
            "Epoch: 030/050 | Batch 0150/0235 | Loss: 36.9717\n",
            "Epoch: 030/050 | Batch 0200/0235 | Loss: 38.5283\n",
            "Time elapsed: 6.80 min\n",
            "Epoch: 031/050 | Batch 0000/0235 | Loss: 35.4061\n",
            "Epoch: 031/050 | Batch 0050/0235 | Loss: 38.8136\n",
            "Epoch: 031/050 | Batch 0100/0235 | Loss: 35.9879\n",
            "Epoch: 031/050 | Batch 0150/0235 | Loss: 36.8183\n",
            "Epoch: 031/050 | Batch 0200/0235 | Loss: 38.5130\n",
            "Time elapsed: 6.97 min\n",
            "Epoch: 032/050 | Batch 0000/0235 | Loss: 35.2173\n",
            "Epoch: 032/050 | Batch 0050/0235 | Loss: 38.7341\n",
            "Epoch: 032/050 | Batch 0100/0235 | Loss: 35.7375\n",
            "Epoch: 032/050 | Batch 0150/0235 | Loss: 36.9761\n",
            "Epoch: 032/050 | Batch 0200/0235 | Loss: 38.9055\n",
            "Time elapsed: 7.14 min\n",
            "Epoch: 033/050 | Batch 0000/0235 | Loss: 35.1353\n",
            "Epoch: 033/050 | Batch 0050/0235 | Loss: 38.7737\n",
            "Epoch: 033/050 | Batch 0100/0235 | Loss: 35.6617\n",
            "Epoch: 033/050 | Batch 0150/0235 | Loss: 36.5674\n",
            "Epoch: 033/050 | Batch 0200/0235 | Loss: 38.5233\n",
            "Time elapsed: 7.31 min\n",
            "Epoch: 034/050 | Batch 0000/0235 | Loss: 35.1397\n",
            "Epoch: 034/050 | Batch 0050/0235 | Loss: 38.7020\n",
            "Epoch: 034/050 | Batch 0100/0235 | Loss: 35.6633\n",
            "Epoch: 034/050 | Batch 0150/0235 | Loss: 36.5652\n",
            "Epoch: 034/050 | Batch 0200/0235 | Loss: 38.3957\n",
            "Time elapsed: 7.48 min\n",
            "Epoch: 035/050 | Batch 0000/0235 | Loss: 35.0673\n",
            "Epoch: 035/050 | Batch 0050/0235 | Loss: 38.8911\n",
            "Epoch: 035/050 | Batch 0100/0235 | Loss: 35.8387\n",
            "Epoch: 035/050 | Batch 0150/0235 | Loss: 36.9084\n",
            "Epoch: 035/050 | Batch 0200/0235 | Loss: 38.3558\n",
            "Time elapsed: 7.65 min\n",
            "Epoch: 036/050 | Batch 0000/0235 | Loss: 34.8200\n",
            "Epoch: 036/050 | Batch 0050/0235 | Loss: 38.7723\n",
            "Epoch: 036/050 | Batch 0100/0235 | Loss: 35.7068\n",
            "Epoch: 036/050 | Batch 0150/0235 | Loss: 36.3560\n",
            "Epoch: 036/050 | Batch 0200/0235 | Loss: 38.1847\n",
            "Time elapsed: 7.82 min\n",
            "Epoch: 037/050 | Batch 0000/0235 | Loss: 35.0486\n",
            "Epoch: 037/050 | Batch 0050/0235 | Loss: 38.2977\n",
            "Epoch: 037/050 | Batch 0100/0235 | Loss: 35.5212\n",
            "Epoch: 037/050 | Batch 0150/0235 | Loss: 36.6811\n",
            "Epoch: 037/050 | Batch 0200/0235 | Loss: 38.3245\n",
            "Time elapsed: 7.99 min\n",
            "Epoch: 038/050 | Batch 0000/0235 | Loss: 35.0332\n",
            "Epoch: 038/050 | Batch 0050/0235 | Loss: 38.5548\n",
            "Epoch: 038/050 | Batch 0100/0235 | Loss: 35.6158\n",
            "Epoch: 038/050 | Batch 0150/0235 | Loss: 36.6419\n",
            "Epoch: 038/050 | Batch 0200/0235 | Loss: 38.2872\n",
            "Time elapsed: 8.17 min\n",
            "Epoch: 039/050 | Batch 0000/0235 | Loss: 34.8770\n",
            "Epoch: 039/050 | Batch 0050/0235 | Loss: 38.4263\n",
            "Epoch: 039/050 | Batch 0100/0235 | Loss: 35.6584\n",
            "Epoch: 039/050 | Batch 0150/0235 | Loss: 36.4249\n",
            "Epoch: 039/050 | Batch 0200/0235 | Loss: 38.1701\n",
            "Time elapsed: 8.35 min\n",
            "Epoch: 040/050 | Batch 0000/0235 | Loss: 34.9423\n",
            "Epoch: 040/050 | Batch 0050/0235 | Loss: 38.4322\n",
            "Epoch: 040/050 | Batch 0100/0235 | Loss: 35.3447\n",
            "Epoch: 040/050 | Batch 0150/0235 | Loss: 36.1650\n",
            "Epoch: 040/050 | Batch 0200/0235 | Loss: 38.3060\n",
            "Time elapsed: 8.52 min\n",
            "Epoch: 041/050 | Batch 0000/0235 | Loss: 34.5033\n",
            "Epoch: 041/050 | Batch 0050/0235 | Loss: 38.4872\n",
            "Epoch: 041/050 | Batch 0100/0235 | Loss: 35.4220\n",
            "Epoch: 041/050 | Batch 0150/0235 | Loss: 36.2004\n",
            "Epoch: 041/050 | Batch 0200/0235 | Loss: 38.1541\n",
            "Time elapsed: 8.69 min\n",
            "Epoch: 042/050 | Batch 0000/0235 | Loss: 34.7173\n",
            "Epoch: 042/050 | Batch 0050/0235 | Loss: 38.1161\n",
            "Epoch: 042/050 | Batch 0100/0235 | Loss: 35.4334\n",
            "Epoch: 042/050 | Batch 0150/0235 | Loss: 36.2884\n",
            "Epoch: 042/050 | Batch 0200/0235 | Loss: 38.1448\n",
            "Time elapsed: 8.85 min\n",
            "Epoch: 043/050 | Batch 0000/0235 | Loss: 34.7832\n",
            "Epoch: 043/050 | Batch 0050/0235 | Loss: 38.7845\n",
            "Epoch: 043/050 | Batch 0100/0235 | Loss: 35.3025\n",
            "Epoch: 043/050 | Batch 0150/0235 | Loss: 36.2694\n",
            "Epoch: 043/050 | Batch 0200/0235 | Loss: 37.9898\n",
            "Time elapsed: 9.02 min\n",
            "Epoch: 044/050 | Batch 0000/0235 | Loss: 34.4279\n",
            "Epoch: 044/050 | Batch 0050/0235 | Loss: 38.3225\n",
            "Epoch: 044/050 | Batch 0100/0235 | Loss: 35.2285\n",
            "Epoch: 044/050 | Batch 0150/0235 | Loss: 36.5478\n",
            "Epoch: 044/050 | Batch 0200/0235 | Loss: 37.8875\n",
            "Time elapsed: 9.20 min\n",
            "Epoch: 045/050 | Batch 0000/0235 | Loss: 34.8110\n",
            "Epoch: 045/050 | Batch 0050/0235 | Loss: 38.7408\n",
            "Epoch: 045/050 | Batch 0100/0235 | Loss: 35.1971\n",
            "Epoch: 045/050 | Batch 0150/0235 | Loss: 36.1110\n",
            "Epoch: 045/050 | Batch 0200/0235 | Loss: 38.1688\n",
            "Time elapsed: 9.37 min\n",
            "Epoch: 046/050 | Batch 0000/0235 | Loss: 34.7627\n",
            "Epoch: 046/050 | Batch 0050/0235 | Loss: 38.5357\n",
            "Epoch: 046/050 | Batch 0100/0235 | Loss: 35.4398\n",
            "Epoch: 046/050 | Batch 0150/0235 | Loss: 36.2561\n",
            "Epoch: 046/050 | Batch 0200/0235 | Loss: 37.7697\n",
            "Time elapsed: 9.54 min\n",
            "Epoch: 047/050 | Batch 0000/0235 | Loss: 34.5338\n",
            "Epoch: 047/050 | Batch 0050/0235 | Loss: 38.6046\n",
            "Epoch: 047/050 | Batch 0100/0235 | Loss: 35.1973\n",
            "Epoch: 047/050 | Batch 0150/0235 | Loss: 36.2205\n",
            "Epoch: 047/050 | Batch 0200/0235 | Loss: 38.0320\n",
            "Time elapsed: 9.70 min\n",
            "Epoch: 048/050 | Batch 0000/0235 | Loss: 34.3337\n",
            "Epoch: 048/050 | Batch 0050/0235 | Loss: 38.4075\n",
            "Epoch: 048/050 | Batch 0100/0235 | Loss: 35.2708\n",
            "Epoch: 048/050 | Batch 0150/0235 | Loss: 35.9144\n",
            "Epoch: 048/050 | Batch 0200/0235 | Loss: 37.6455\n",
            "Time elapsed: 9.88 min\n",
            "Epoch: 049/050 | Batch 0000/0235 | Loss: 34.2773\n",
            "Epoch: 049/050 | Batch 0050/0235 | Loss: 38.7551\n",
            "Epoch: 049/050 | Batch 0100/0235 | Loss: 35.1674\n",
            "Epoch: 049/050 | Batch 0150/0235 | Loss: 36.1937\n",
            "Epoch: 049/050 | Batch 0200/0235 | Loss: 37.5851\n",
            "Time elapsed: 10.05 min\n",
            "Epoch: 050/050 | Batch 0000/0235 | Loss: 34.6730\n",
            "Epoch: 050/050 | Batch 0050/0235 | Loss: 38.1502\n",
            "Epoch: 050/050 | Batch 0100/0235 | Loss: 35.6502\n",
            "Epoch: 050/050 | Batch 0150/0235 | Loss: 36.1171\n",
            "Epoch: 050/050 | Batch 0200/0235 | Loss: 37.6736\n",
            "Time elapsed: 10.22 min\n",
            "Total Training Time: 10.22 min\n"
          ]
        }
      ],
      "source": [
        "#instantiate VAE\n",
        "torch.manual_seed(2342)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=VAE()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0005)\n",
        "save_path='/content/drive/MyDrive/model_params/model_vae.pt'\n",
        "\n",
        "log_dict = train_vae(num_epochs=50, model=model,\n",
        "                        optimizer=optimizer, device=device,\n",
        "                        train_loader=train_loader,\n",
        "                        skip_epoch_stats=True,\n",
        "                        logging_interval=50,\n",
        "                     save_model=save_path)\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the list of tensors to a file\n",
        "with open(r'/content/drive/MyDrive/model_params/log_dict_vae_mnist.pkl', 'wb') as file:\n",
        "    pickle.dump(log_dict, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHQTVcVJHPZw"
      },
      "id": "cHQTVcVJHPZw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}