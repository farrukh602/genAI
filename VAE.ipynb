{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "add527d1-2e52-4f67-8a98-6e06de83211e",
      "metadata": {
        "id": "add527d1-2e52-4f67-8a98-6e06de83211e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "34321f38-8983-4630-a452-23934300aa27",
      "metadata": {
        "id": "34321f38-8983-4630-a452-23934300aa27"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='data',train=True, transform=transform,download=True)\n",
        "test_dataset = datasets.MNIST(root='data',train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MOXsoRlL-yKL",
        "outputId": "5e5487c5-df3e-4ade-f662-7a267c859214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MOXsoRlL-yKL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "edd808da-7b7f-49fc-9209-37498cea2e4d",
      "metadata": {
        "id": "edd808da-7b7f-49fc-9209-37498cea2e4d",
        "outputId": "4144e442-6104-4325-a39f-b5a7cf927825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "\n",
            "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([256])\n",
            "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n"
          ]
        }
      ],
      "source": [
        "# Checking the dataset\n",
        "print('Training Set:\\n')\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.size())\n",
        "    print('Image label dimensions:', labels.size())\n",
        "    print(labels[:10])\n",
        "    break\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc19b970-cf75-46d7-97f3-ef3b00644ef3",
      "metadata": {
        "id": "fc19b970-cf75-46d7-97f3-ef3b00644ef3"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "630899e6-24ca-400a-a521-e8b52a38ac1e",
      "metadata": {
        "id": "630899e6-24ca-400a-a521-e8b52a38ac1e"
      },
      "outputs": [],
      "source": [
        "# Custom reshape class\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self,*args):\n",
        "        super(Reshape,self).__init__()\n",
        "        self.shape = args\n",
        "    def forward(self,x):\n",
        "        with_new_shape = x.view(self.shape)\n",
        "        return with_new_shape\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self,*args):\n",
        "        super(Trim, self).__init__()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x[:, :, :28, :28]\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE,self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,32,3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(32,64,3,stride=2,padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64,64, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64,64, 3, stride=1, padding=1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.z_mean = torch.nn.Linear(3136,2)\n",
        "        self.z_log_var = torch.nn.Linear(3136,2)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2,3136),\n",
        "            Reshape(-1,64,7,7),\n",
        "            nn.ConvTranspose2d(64,64, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(64,64,3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(64,32,3, stride=2, padding=0),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(32,1,3 ,stride=1, padding=0),\n",
        "            Trim(),\n",
        "            nn.Sigmoid()\n",
        "\n",
        "\n",
        "        )\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        eps = torch.randn(mean.size(0),mean.size(1)).to(mean.get_device() )\n",
        "        sigma = torch.exp(log_var/2.0)\n",
        "        z = mean+sigma*eps\n",
        "        return z\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=  x.view((-1,1,28,28))\n",
        "        encoded=self.encoder(x)\n",
        "        mean = self.z_mean(encoded)\n",
        "        log_variance = self.z_log_var(encoded)\n",
        "        encoded_z  = self.reparameterize(mean, log_variance)\n",
        "        decoded=self.decoder(encoded_z)\n",
        "        return encoded, mean, log_variance, decoded\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2b5011-cb5d-43e7-9031-7d3e94959106",
      "metadata": {
        "id": "0d2b5011-cb5d-43e7-9031-7d3e94959106"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ecc37475-c8f5-4d79-8912-fdfde10ff57c",
      "metadata": {
        "id": "ecc37475-c8f5-4d79-8912-fdfde10ff57c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, _ in data_loader:\n",
        "            features = features.to(device)\n",
        "            logits = model(features)\n",
        "            loss = loss_fn(logits, features, reduction='sum')\n",
        "            num_examples += features.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9806d036-4231-498d-b8d4-872c5e221219",
      "metadata": {
        "id": "9806d036-4231-498d-b8d4-872c5e221219"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "log_dict = {'train_combined_loss_per_epoch': [],\n",
        "           'train_combined_loss_per_batch':[],\n",
        "           'train_reconstruction_loss_per_batch':[],\n",
        "           'train_kl_loss_per_batch':[]}\n",
        "loss_fn = F.mse_loss\n",
        "start_time = time.time()\n",
        "def train_vae(num_epochs, model, optimizer, device, train_loader,\n",
        "             loss_fn=None,\n",
        "             logging_interval=100,\n",
        "             skip_epoch_stats=False,\n",
        "             reconstruction_term_weight=1,\n",
        "             save_model=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, _) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "\n",
        "            _, mean, log_var, reconstruction=model(features)\n",
        "            # print(mean.get_device())\n",
        "\n",
        "            # KL Div Loss\n",
        "            kl_loss=torch.sum(-0.5*(1+log_var - mean**2 - torch.exp(log_var)),axis=1)\n",
        "            batch_size=kl_loss.size(0)\n",
        "\n",
        "            kl_loss_avg = kl_loss.mean()\n",
        "             ## Reconstruction Loss\n",
        "            if loss_fn is None:\n",
        "              loss_fn = F.mse_loss\n",
        "            # print(reconstruction.get_device())\n",
        "            # print(features.get_device())\n",
        "            reconstruction_loss = loss_fn(reconstruction, features,reduction='none')\n",
        "            reconstruction_loss = reconstruction_loss.view(batch_size, -1).sum(axis=1)\n",
        "            reconstruction_loss=reconstruction_loss.mean()\n",
        "\n",
        "            total_loss = reconstruction_loss+kl_loss_avg\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # LOGGING\n",
        "            log_dict['train_combined_loss_per_batch'].append(total_loss.item())\n",
        "            log_dict['train_reconstruction_loss_per_batch'].append(reconstruction_loss.item())\n",
        "            log_dict['train_kl_loss_per_batch'].append(kl_loss_avg.item())\n",
        "\n",
        "            if not batch_idx % logging_interval:\n",
        "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
        "                          % (epoch+1, num_epochs, batch_idx,\n",
        "                              len(train_loader), total_loss))\n",
        "        if not skip_epoch_stats:\n",
        "                model.eval()\n",
        "\n",
        "                with torch.set_grad_enabled(False):  # save memory during inference\n",
        "\n",
        "                    train_loss = compute_epoch_loss_autoencoder(\n",
        "                        model, train_loader, loss_fn, device)\n",
        "                    print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
        "                          epoch+1, num_epochs, train_loss))\n",
        "                    log_dict['train_combined_per_epoch'].append(train_loss.item())\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    if save_model is not None:\n",
        "        torch.save(model.state_dict(), save_model)\n",
        "\n",
        "    return log_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "4f128837-2bf0-47b7-affb-718e104ca3f7",
      "metadata": {
        "id": "4f128837-2bf0-47b7-affb-718e104ca3f7",
        "outputId": "115b95b4-cbe0-41d6-b82e-c8dd426af5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/050 | Batch 0000/0235 | Loss: 225.4168\n",
            "Epoch: 001/050 | Batch 0050/0235 | Loss: 58.2656\n",
            "Epoch: 001/050 | Batch 0100/0235 | Loss: 52.6175\n",
            "Epoch: 001/050 | Batch 0150/0235 | Loss: 52.2990\n",
            "Epoch: 001/050 | Batch 0200/0235 | Loss: 51.2735\n",
            "Time elapsed: 0.27 min\n",
            "Epoch: 002/050 | Batch 0000/0235 | Loss: 46.5014\n",
            "Epoch: 002/050 | Batch 0050/0235 | Loss: 47.0842\n",
            "Epoch: 002/050 | Batch 0100/0235 | Loss: 43.0643\n",
            "Epoch: 002/050 | Batch 0150/0235 | Loss: 44.6235\n",
            "Epoch: 002/050 | Batch 0200/0235 | Loss: 46.3614\n",
            "Time elapsed: 0.43 min\n",
            "Epoch: 003/050 | Batch 0000/0235 | Loss: 42.7619\n",
            "Epoch: 003/050 | Batch 0050/0235 | Loss: 44.8465\n",
            "Epoch: 003/050 | Batch 0100/0235 | Loss: 41.0364\n",
            "Epoch: 003/050 | Batch 0150/0235 | Loss: 43.3199\n",
            "Epoch: 003/050 | Batch 0200/0235 | Loss: 44.8372\n",
            "Time elapsed: 0.60 min\n",
            "Epoch: 004/050 | Batch 0000/0235 | Loss: 41.5720\n",
            "Epoch: 004/050 | Batch 0050/0235 | Loss: 43.5364\n",
            "Epoch: 004/050 | Batch 0100/0235 | Loss: 40.3312\n",
            "Epoch: 004/050 | Batch 0150/0235 | Loss: 42.7747\n",
            "Epoch: 004/050 | Batch 0200/0235 | Loss: 43.7446\n",
            "Time elapsed: 0.78 min\n",
            "Epoch: 005/050 | Batch 0000/0235 | Loss: 40.8690\n",
            "Epoch: 005/050 | Batch 0050/0235 | Loss: 42.6019\n",
            "Epoch: 005/050 | Batch 0100/0235 | Loss: 39.4730\n",
            "Epoch: 005/050 | Batch 0150/0235 | Loss: 41.6902\n",
            "Epoch: 005/050 | Batch 0200/0235 | Loss: 43.0193\n",
            "Time elapsed: 0.95 min\n",
            "Epoch: 006/050 | Batch 0000/0235 | Loss: 39.8703\n",
            "Epoch: 006/050 | Batch 0050/0235 | Loss: 42.2571\n",
            "Epoch: 006/050 | Batch 0100/0235 | Loss: 38.8153\n",
            "Epoch: 006/050 | Batch 0150/0235 | Loss: 41.4533\n",
            "Epoch: 006/050 | Batch 0200/0235 | Loss: 42.1331\n",
            "Time elapsed: 1.11 min\n",
            "Epoch: 007/050 | Batch 0000/0235 | Loss: 39.7798\n",
            "Epoch: 007/050 | Batch 0050/0235 | Loss: 42.0633\n",
            "Epoch: 007/050 | Batch 0100/0235 | Loss: 38.2954\n",
            "Epoch: 007/050 | Batch 0150/0235 | Loss: 40.8269\n",
            "Epoch: 007/050 | Batch 0200/0235 | Loss: 41.8663\n",
            "Time elapsed: 1.28 min\n",
            "Epoch: 008/050 | Batch 0000/0235 | Loss: 39.0548\n",
            "Epoch: 008/050 | Batch 0050/0235 | Loss: 41.8762\n",
            "Epoch: 008/050 | Batch 0100/0235 | Loss: 37.8753\n",
            "Epoch: 008/050 | Batch 0150/0235 | Loss: 39.8889\n",
            "Epoch: 008/050 | Batch 0200/0235 | Loss: 41.4649\n",
            "Time elapsed: 1.46 min\n",
            "Epoch: 009/050 | Batch 0000/0235 | Loss: 39.1036\n",
            "Epoch: 009/050 | Batch 0050/0235 | Loss: 41.3903\n",
            "Epoch: 009/050 | Batch 0100/0235 | Loss: 37.6130\n",
            "Epoch: 009/050 | Batch 0150/0235 | Loss: 39.5560\n",
            "Epoch: 009/050 | Batch 0200/0235 | Loss: 41.4414\n",
            "Time elapsed: 1.63 min\n",
            "Epoch: 010/050 | Batch 0000/0235 | Loss: 38.8473\n",
            "Epoch: 010/050 | Batch 0050/0235 | Loss: 41.1302\n",
            "Epoch: 010/050 | Batch 0100/0235 | Loss: 37.4256\n",
            "Epoch: 010/050 | Batch 0150/0235 | Loss: 39.6486\n",
            "Epoch: 010/050 | Batch 0200/0235 | Loss: 40.7116\n",
            "Time elapsed: 1.82 min\n",
            "Epoch: 011/050 | Batch 0000/0235 | Loss: 38.6665\n",
            "Epoch: 011/050 | Batch 0050/0235 | Loss: 41.1322\n",
            "Epoch: 011/050 | Batch 0100/0235 | Loss: 37.0578\n",
            "Epoch: 011/050 | Batch 0150/0235 | Loss: 39.2927\n",
            "Epoch: 011/050 | Batch 0200/0235 | Loss: 40.6836\n",
            "Time elapsed: 1.98 min\n",
            "Epoch: 012/050 | Batch 0000/0235 | Loss: 37.9051\n",
            "Epoch: 012/050 | Batch 0050/0235 | Loss: 40.9283\n",
            "Epoch: 012/050 | Batch 0100/0235 | Loss: 36.9911\n",
            "Epoch: 012/050 | Batch 0150/0235 | Loss: 39.2699\n",
            "Epoch: 012/050 | Batch 0200/0235 | Loss: 40.5744\n",
            "Time elapsed: 2.15 min\n",
            "Epoch: 013/050 | Batch 0000/0235 | Loss: 37.4434\n",
            "Epoch: 013/050 | Batch 0050/0235 | Loss: 40.8194\n",
            "Epoch: 013/050 | Batch 0100/0235 | Loss: 36.9947\n",
            "Epoch: 013/050 | Batch 0150/0235 | Loss: 39.0746\n",
            "Epoch: 013/050 | Batch 0200/0235 | Loss: 40.6208\n",
            "Time elapsed: 2.33 min\n",
            "Epoch: 014/050 | Batch 0000/0235 | Loss: 37.5777\n",
            "Epoch: 014/050 | Batch 0050/0235 | Loss: 40.4024\n",
            "Epoch: 014/050 | Batch 0100/0235 | Loss: 36.8785\n",
            "Epoch: 014/050 | Batch 0150/0235 | Loss: 38.8152\n",
            "Epoch: 014/050 | Batch 0200/0235 | Loss: 40.2470\n",
            "Time elapsed: 2.50 min\n",
            "Epoch: 015/050 | Batch 0000/0235 | Loss: 37.1753\n",
            "Epoch: 015/050 | Batch 0050/0235 | Loss: 40.0478\n",
            "Epoch: 015/050 | Batch 0100/0235 | Loss: 36.9169\n",
            "Epoch: 015/050 | Batch 0150/0235 | Loss: 38.4073\n",
            "Epoch: 015/050 | Batch 0200/0235 | Loss: 40.2605\n",
            "Time elapsed: 2.66 min\n",
            "Epoch: 016/050 | Batch 0000/0235 | Loss: 36.8860\n",
            "Epoch: 016/050 | Batch 0050/0235 | Loss: 39.8426\n",
            "Epoch: 016/050 | Batch 0100/0235 | Loss: 36.7066\n",
            "Epoch: 016/050 | Batch 0150/0235 | Loss: 38.4947\n",
            "Epoch: 016/050 | Batch 0200/0235 | Loss: 40.3153\n",
            "Time elapsed: 2.83 min\n",
            "Epoch: 017/050 | Batch 0000/0235 | Loss: 36.6204\n",
            "Epoch: 017/050 | Batch 0050/0235 | Loss: 39.9274\n",
            "Epoch: 017/050 | Batch 0100/0235 | Loss: 36.3896\n",
            "Epoch: 017/050 | Batch 0150/0235 | Loss: 38.5173\n",
            "Epoch: 017/050 | Batch 0200/0235 | Loss: 39.9323\n",
            "Time elapsed: 3.01 min\n",
            "Epoch: 018/050 | Batch 0000/0235 | Loss: 36.6739\n",
            "Epoch: 018/050 | Batch 0050/0235 | Loss: 39.7888\n",
            "Epoch: 018/050 | Batch 0100/0235 | Loss: 36.3712\n",
            "Epoch: 018/050 | Batch 0150/0235 | Loss: 37.9659\n",
            "Epoch: 018/050 | Batch 0200/0235 | Loss: 39.8822\n",
            "Time elapsed: 3.18 min\n",
            "Epoch: 019/050 | Batch 0000/0235 | Loss: 36.5527\n",
            "Epoch: 019/050 | Batch 0050/0235 | Loss: 40.1235\n",
            "Epoch: 019/050 | Batch 0100/0235 | Loss: 36.5933\n",
            "Epoch: 019/050 | Batch 0150/0235 | Loss: 38.5056\n",
            "Epoch: 019/050 | Batch 0200/0235 | Loss: 39.8679\n",
            "Time elapsed: 3.35 min\n",
            "Epoch: 020/050 | Batch 0000/0235 | Loss: 36.1248\n",
            "Epoch: 020/050 | Batch 0050/0235 | Loss: 39.8717\n",
            "Epoch: 020/050 | Batch 0100/0235 | Loss: 36.1352\n",
            "Epoch: 020/050 | Batch 0150/0235 | Loss: 38.3026\n",
            "Epoch: 020/050 | Batch 0200/0235 | Loss: 39.8432\n",
            "Time elapsed: 3.51 min\n",
            "Epoch: 021/050 | Batch 0000/0235 | Loss: 36.2754\n",
            "Epoch: 021/050 | Batch 0050/0235 | Loss: 39.9292\n",
            "Epoch: 021/050 | Batch 0100/0235 | Loss: 35.8499\n",
            "Epoch: 021/050 | Batch 0150/0235 | Loss: 38.1599\n",
            "Epoch: 021/050 | Batch 0200/0235 | Loss: 39.7314\n",
            "Time elapsed: 3.69 min\n",
            "Epoch: 022/050 | Batch 0000/0235 | Loss: 36.0431\n",
            "Epoch: 022/050 | Batch 0050/0235 | Loss: 39.6964\n",
            "Epoch: 022/050 | Batch 0100/0235 | Loss: 36.1821\n",
            "Epoch: 022/050 | Batch 0150/0235 | Loss: 38.2683\n",
            "Epoch: 022/050 | Batch 0200/0235 | Loss: 39.0815\n",
            "Time elapsed: 3.86 min\n",
            "Epoch: 023/050 | Batch 0000/0235 | Loss: 35.7219\n",
            "Epoch: 023/050 | Batch 0050/0235 | Loss: 39.6712\n",
            "Epoch: 023/050 | Batch 0100/0235 | Loss: 36.1793\n",
            "Epoch: 023/050 | Batch 0150/0235 | Loss: 38.1067\n",
            "Epoch: 023/050 | Batch 0200/0235 | Loss: 39.5181\n",
            "Time elapsed: 4.04 min\n",
            "Epoch: 024/050 | Batch 0000/0235 | Loss: 35.9829\n",
            "Epoch: 024/050 | Batch 0050/0235 | Loss: 39.5103\n",
            "Epoch: 024/050 | Batch 0100/0235 | Loss: 35.9815\n",
            "Epoch: 024/050 | Batch 0150/0235 | Loss: 37.9190\n",
            "Epoch: 024/050 | Batch 0200/0235 | Loss: 39.2578\n",
            "Time elapsed: 4.20 min\n",
            "Epoch: 025/050 | Batch 0000/0235 | Loss: 35.5983\n",
            "Epoch: 025/050 | Batch 0050/0235 | Loss: 39.2009\n",
            "Epoch: 025/050 | Batch 0100/0235 | Loss: 35.8057\n",
            "Epoch: 025/050 | Batch 0150/0235 | Loss: 37.8479\n",
            "Epoch: 025/050 | Batch 0200/0235 | Loss: 39.1775\n",
            "Time elapsed: 4.37 min\n",
            "Epoch: 026/050 | Batch 0000/0235 | Loss: 35.3573\n",
            "Epoch: 026/050 | Batch 0050/0235 | Loss: 39.0894\n",
            "Epoch: 026/050 | Batch 0100/0235 | Loss: 36.0109\n",
            "Epoch: 026/050 | Batch 0150/0235 | Loss: 38.2289\n",
            "Epoch: 026/050 | Batch 0200/0235 | Loss: 39.0941\n",
            "Time elapsed: 4.55 min\n",
            "Epoch: 027/050 | Batch 0000/0235 | Loss: 35.5254\n",
            "Epoch: 027/050 | Batch 0050/0235 | Loss: 39.8190\n",
            "Epoch: 027/050 | Batch 0100/0235 | Loss: 35.8115\n",
            "Epoch: 027/050 | Batch 0150/0235 | Loss: 37.7790\n",
            "Epoch: 027/050 | Batch 0200/0235 | Loss: 39.1689\n",
            "Time elapsed: 4.72 min\n",
            "Epoch: 028/050 | Batch 0000/0235 | Loss: 35.6058\n",
            "Epoch: 028/050 | Batch 0050/0235 | Loss: 39.2946\n",
            "Epoch: 028/050 | Batch 0100/0235 | Loss: 35.5275\n",
            "Epoch: 028/050 | Batch 0150/0235 | Loss: 37.8273\n",
            "Epoch: 028/050 | Batch 0200/0235 | Loss: 38.9110\n",
            "Time elapsed: 4.88 min\n",
            "Epoch: 029/050 | Batch 0000/0235 | Loss: 35.5941\n",
            "Epoch: 029/050 | Batch 0050/0235 | Loss: 38.9764\n",
            "Epoch: 029/050 | Batch 0100/0235 | Loss: 35.5631\n",
            "Epoch: 029/050 | Batch 0150/0235 | Loss: 37.4600\n",
            "Epoch: 029/050 | Batch 0200/0235 | Loss: 38.8039\n",
            "Time elapsed: 5.05 min\n",
            "Epoch: 030/050 | Batch 0000/0235 | Loss: 35.0587\n",
            "Epoch: 030/050 | Batch 0050/0235 | Loss: 38.8040\n",
            "Epoch: 030/050 | Batch 0100/0235 | Loss: 35.5078\n",
            "Epoch: 030/050 | Batch 0150/0235 | Loss: 37.8447\n",
            "Epoch: 030/050 | Batch 0200/0235 | Loss: 38.8191\n",
            "Time elapsed: 5.23 min\n",
            "Epoch: 031/050 | Batch 0000/0235 | Loss: 35.2416\n",
            "Epoch: 031/050 | Batch 0050/0235 | Loss: 38.9895\n",
            "Epoch: 031/050 | Batch 0100/0235 | Loss: 35.8676\n",
            "Epoch: 031/050 | Batch 0150/0235 | Loss: 37.6149\n",
            "Epoch: 031/050 | Batch 0200/0235 | Loss: 38.5335\n",
            "Time elapsed: 5.40 min\n",
            "Epoch: 032/050 | Batch 0000/0235 | Loss: 35.1366\n",
            "Epoch: 032/050 | Batch 0050/0235 | Loss: 39.0696\n",
            "Epoch: 032/050 | Batch 0100/0235 | Loss: 35.6933\n",
            "Epoch: 032/050 | Batch 0150/0235 | Loss: 37.7096\n",
            "Epoch: 032/050 | Batch 0200/0235 | Loss: 38.9285\n",
            "Time elapsed: 5.57 min\n",
            "Epoch: 033/050 | Batch 0000/0235 | Loss: 35.3212\n",
            "Epoch: 033/050 | Batch 0050/0235 | Loss: 38.7327\n",
            "Epoch: 033/050 | Batch 0100/0235 | Loss: 35.6331\n",
            "Epoch: 033/050 | Batch 0150/0235 | Loss: 37.4759\n",
            "Epoch: 033/050 | Batch 0200/0235 | Loss: 38.7459\n",
            "Time elapsed: 5.73 min\n",
            "Epoch: 034/050 | Batch 0000/0235 | Loss: 34.9607\n",
            "Epoch: 034/050 | Batch 0050/0235 | Loss: 38.9056\n",
            "Epoch: 034/050 | Batch 0100/0235 | Loss: 35.7170\n",
            "Epoch: 034/050 | Batch 0150/0235 | Loss: 37.2822\n",
            "Epoch: 034/050 | Batch 0200/0235 | Loss: 38.3281\n",
            "Time elapsed: 5.91 min\n",
            "Epoch: 035/050 | Batch 0000/0235 | Loss: 34.8839\n",
            "Epoch: 035/050 | Batch 0050/0235 | Loss: 38.9764\n",
            "Epoch: 035/050 | Batch 0100/0235 | Loss: 35.8257\n",
            "Epoch: 035/050 | Batch 0150/0235 | Loss: 37.5050\n",
            "Epoch: 035/050 | Batch 0200/0235 | Loss: 38.3256\n",
            "Time elapsed: 6.08 min\n",
            "Epoch: 036/050 | Batch 0000/0235 | Loss: 34.8512\n",
            "Epoch: 036/050 | Batch 0050/0235 | Loss: 38.7020\n",
            "Epoch: 036/050 | Batch 0100/0235 | Loss: 35.6878\n",
            "Epoch: 036/050 | Batch 0150/0235 | Loss: 37.5052\n",
            "Epoch: 036/050 | Batch 0200/0235 | Loss: 38.3063\n",
            "Time elapsed: 6.28 min\n",
            "Epoch: 037/050 | Batch 0000/0235 | Loss: 34.9304\n",
            "Epoch: 037/050 | Batch 0050/0235 | Loss: 38.2793\n",
            "Epoch: 037/050 | Batch 0100/0235 | Loss: 35.7051\n",
            "Epoch: 037/050 | Batch 0150/0235 | Loss: 37.5469\n",
            "Epoch: 037/050 | Batch 0200/0235 | Loss: 38.2780\n",
            "Time elapsed: 6.44 min\n",
            "Epoch: 038/050 | Batch 0000/0235 | Loss: 35.0121\n",
            "Epoch: 038/050 | Batch 0050/0235 | Loss: 38.4615\n",
            "Epoch: 038/050 | Batch 0100/0235 | Loss: 35.5374\n",
            "Epoch: 038/050 | Batch 0150/0235 | Loss: 37.4033\n",
            "Epoch: 038/050 | Batch 0200/0235 | Loss: 38.3180\n",
            "Time elapsed: 6.61 min\n",
            "Epoch: 039/050 | Batch 0000/0235 | Loss: 34.6552\n",
            "Epoch: 039/050 | Batch 0050/0235 | Loss: 38.5125\n",
            "Epoch: 039/050 | Batch 0100/0235 | Loss: 35.7210\n",
            "Epoch: 039/050 | Batch 0150/0235 | Loss: 37.2904\n",
            "Epoch: 039/050 | Batch 0200/0235 | Loss: 38.1185\n",
            "Time elapsed: 6.79 min\n",
            "Epoch: 040/050 | Batch 0000/0235 | Loss: 34.7832\n",
            "Epoch: 040/050 | Batch 0050/0235 | Loss: 38.4399\n",
            "Epoch: 040/050 | Batch 0100/0235 | Loss: 35.3029\n",
            "Epoch: 040/050 | Batch 0150/0235 | Loss: 36.8417\n",
            "Epoch: 040/050 | Batch 0200/0235 | Loss: 38.3284\n",
            "Time elapsed: 6.96 min\n",
            "Epoch: 041/050 | Batch 0000/0235 | Loss: 34.9942\n",
            "Epoch: 041/050 | Batch 0050/0235 | Loss: 38.5386\n",
            "Epoch: 041/050 | Batch 0100/0235 | Loss: 35.5320\n",
            "Epoch: 041/050 | Batch 0150/0235 | Loss: 37.0896\n",
            "Epoch: 041/050 | Batch 0200/0235 | Loss: 37.9154\n",
            "Time elapsed: 7.13 min\n",
            "Epoch: 042/050 | Batch 0000/0235 | Loss: 34.7937\n",
            "Epoch: 042/050 | Batch 0050/0235 | Loss: 38.3062\n",
            "Epoch: 042/050 | Batch 0100/0235 | Loss: 35.4594\n",
            "Epoch: 042/050 | Batch 0150/0235 | Loss: 37.1891\n",
            "Epoch: 042/050 | Batch 0200/0235 | Loss: 38.1730\n",
            "Time elapsed: 7.30 min\n",
            "Epoch: 043/050 | Batch 0000/0235 | Loss: 34.8346\n",
            "Epoch: 043/050 | Batch 0050/0235 | Loss: 38.4644\n",
            "Epoch: 043/050 | Batch 0100/0235 | Loss: 35.4380\n",
            "Epoch: 043/050 | Batch 0150/0235 | Loss: 36.8998\n",
            "Epoch: 043/050 | Batch 0200/0235 | Loss: 37.9652\n",
            "Time elapsed: 7.47 min\n",
            "Epoch: 044/050 | Batch 0000/0235 | Loss: 34.5794\n",
            "Epoch: 044/050 | Batch 0050/0235 | Loss: 38.1826\n",
            "Epoch: 044/050 | Batch 0100/0235 | Loss: 35.2235\n",
            "Epoch: 044/050 | Batch 0150/0235 | Loss: 37.2491\n",
            "Epoch: 044/050 | Batch 0200/0235 | Loss: 37.9180\n",
            "Time elapsed: 7.64 min\n",
            "Epoch: 045/050 | Batch 0000/0235 | Loss: 34.7199\n",
            "Epoch: 045/050 | Batch 0050/0235 | Loss: 38.4534\n",
            "Epoch: 045/050 | Batch 0100/0235 | Loss: 35.3823\n",
            "Epoch: 045/050 | Batch 0150/0235 | Loss: 36.8752\n",
            "Epoch: 045/050 | Batch 0200/0235 | Loss: 38.0375\n",
            "Time elapsed: 7.80 min\n",
            "Epoch: 046/050 | Batch 0000/0235 | Loss: 34.5028\n",
            "Epoch: 046/050 | Batch 0050/0235 | Loss: 38.3735\n",
            "Epoch: 046/050 | Batch 0100/0235 | Loss: 35.5532\n",
            "Epoch: 046/050 | Batch 0150/0235 | Loss: 37.1119\n",
            "Epoch: 046/050 | Batch 0200/0235 | Loss: 37.5530\n",
            "Time elapsed: 7.96 min\n",
            "Epoch: 047/050 | Batch 0000/0235 | Loss: 34.7362\n",
            "Epoch: 047/050 | Batch 0050/0235 | Loss: 38.5469\n",
            "Epoch: 047/050 | Batch 0100/0235 | Loss: 35.3375\n",
            "Epoch: 047/050 | Batch 0150/0235 | Loss: 36.7157\n",
            "Epoch: 047/050 | Batch 0200/0235 | Loss: 37.6870\n",
            "Time elapsed: 8.14 min\n",
            "Epoch: 048/050 | Batch 0000/0235 | Loss: 34.5985\n",
            "Epoch: 048/050 | Batch 0050/0235 | Loss: 38.4312\n",
            "Epoch: 048/050 | Batch 0100/0235 | Loss: 35.5434\n",
            "Epoch: 048/050 | Batch 0150/0235 | Loss: 36.8681\n",
            "Epoch: 048/050 | Batch 0200/0235 | Loss: 37.4938\n",
            "Time elapsed: 8.31 min\n",
            "Epoch: 049/050 | Batch 0000/0235 | Loss: 34.4979\n",
            "Epoch: 049/050 | Batch 0050/0235 | Loss: 38.4566\n",
            "Epoch: 049/050 | Batch 0100/0235 | Loss: 35.6429\n",
            "Epoch: 049/050 | Batch 0150/0235 | Loss: 36.9095\n",
            "Epoch: 049/050 | Batch 0200/0235 | Loss: 37.2417\n",
            "Time elapsed: 8.49 min\n",
            "Epoch: 050/050 | Batch 0000/0235 | Loss: 34.5683\n",
            "Epoch: 050/050 | Batch 0050/0235 | Loss: 37.9964\n",
            "Epoch: 050/050 | Batch 0100/0235 | Loss: 35.2982\n",
            "Epoch: 050/050 | Batch 0150/0235 | Loss: 36.6616\n",
            "Epoch: 050/050 | Batch 0200/0235 | Loss: 37.4875\n",
            "Time elapsed: 8.65 min\n",
            "Total Training Time: 8.65 min\n"
          ]
        }
      ],
      "source": [
        "#instantiate VAE\n",
        "torch.manual_seed(2342)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=VAE()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0005)\n",
        "save_path='/content/drive/MyDrive/model_params/model_vae.pt'\n",
        "\n",
        "log_dict = train_vae(num_epochs=50, model=model,\n",
        "                        optimizer=optimizer, device=device,\n",
        "                        train_loader=train_loader,\n",
        "                        skip_epoch_stats=True,\n",
        "                        logging_interval=50,\n",
        "                     save_model=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHQTVcVJHPZw"
      },
      "id": "cHQTVcVJHPZw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}